# machine_learning_work
# 加载的库：
jieba、sklearn
# 标题：
微博上的文字进行的种类预测

# Work1.py
#1.语料选择
得有已经标记号的语料。
data：原始数据
stop：停词.txt
test1：训练数据
（此数据集直接从网上git得到，数据比较干净）
#2.预处理
数据预处理包含多种方法：数据清理，数据集成，数据变换，数据规约等。
除去噪声，如：格式转换，去掉符号，整体规范化
在本次实验中我们除去了各种符号，制表符，空格，回车等不必要的干扰因素。

除去换行符：
    result = (str(content)).replace("\r\n", “").strip()

#3.中文分词
将文章分割成一个个词语
中文分词就是将一句话拆分为各个词语，因为中文分词在不同的语境中歧义较大，所以分词极其重要。

    原型：我今天中午吃的小面。
    
    分词：我、今天、中午、吃、的、小面。
    
    其中  我、的   两个分词属于停用词。

#4.构建训练集向量空间
统计词频（TF），生成每个文本的词向量空间，并去掉（的、是、在……）等停用词


#5.构建测试集向量空间
方法同上
#6.使用朴素贝叶斯预测文本分类
#7.结果评价
列出预测错误的数据集，以及预测错误率

# Work2.py
同上步骤1,2,3,4,5
训练完后，手动输入相关句子，来判定种类

# 分词算法
jieba分词是基于前缀词典实现的高效词图扫描，生成句子中汉子所有可能成词情况所构成的有向无环图（DAG）
采用动态规划查找最大概率路径，找出基于词频的最大切分组合

1.精确模式，试图将句子最精确的起开，适合文本分析。

2.全模式，把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。

3.搜索引擎模式，再将却模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词

    精确模式: 我/ 去过/ 清华大学/ 和/ 北京大学/ 。

    全模式: 我/ 去过/ 清华/ 清华大学/ 华大/ 大学/ 和/ 北京/ 北京大学/ 大学/ / 

    搜索引擎模式: 我/ 去过/ 清华/ 华大/ 大学/ 清华大学/ 和/ 北京/ 大学/ 北京大学/ 。

我们本次采用jieba分词现将本文分类，然后将分词之后的文本存入一个新的文档中，方便后续函数调用。
分词模式采用的是默认的精准分词模式

    cutResult = jieba.cut(result)  # 默认方式分词，分词结果用空格隔开

# TF-IDF  逆文本频率指数
是一种统计方法，用以评估一个词对于一个语料库中一份文件的重要程度。词的重要性随着在文件中出现的次数正比增加，同时随着它在语料库其他文件中出现的频率反比下降。

就是说一个词在某一文档中出现次数比较多，其他文档没有出现，说明该词对该文档分类很重要。

然而如果其他文档也出现比较多，说明该词区分性不大，就用IDF来降低该词的权重。
    
    步骤:
    
    1.首先是调用已分词的文档
    
    2.构建词频矩阵
    
    3.过滤停用词 
    
    4.将计算好的TF-IDF向量提取出来

TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比

    TF-IDF = TF (词频)   *   IDF(逆文档频率)
    词频：TF = 词在文档中出现的次数 / 文档中总词数
    逆文档频率：IDF = log（语料库中文档总数 / 包含该词的文档数  +1 ）                                          

# 朴素贝叶斯
自行百度

# 手动数据输入：
    例子1：2020年的最好看的书是《三体》。
    例子2：女人需要学会如何去爱自己。
    例子3：缺乏运动的年轻人往往生活习惯不健康。
    例子4：校园暴力是不被允许的。